{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Switch_v2_Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7quwWZTJlwfV",
        "colab_type": "text"
      },
      "source": [
        "# DQN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAePzQjrcriF",
        "colab_type": "text"
      },
      "source": [
        "### Environment Example\n",
        "#### Switch2-v0\n",
        "![Switch-2](https://raw.githubusercontent.com/koulanurag/ma-gym/master/static/gif/Switch2-v0.gif)\n",
        "#### Switch4-v0\n",
        "![Switch-4](https://raw.githubusercontent.com/koulanurag/ma-gym/master/static/gif/Switch4-v0.gif)\n",
        "\n",
        "`Switch-n` is a grid world environment having `n agents` where each agent wants to move their corresponding home location (marked in boxes outlined in same colors).\n",
        "Each agent receives only it's local position coordinates. The challenging part of the game is to pass through the narrow corridor through which only one agent can pass at a time. They need to coordinate to not block the pathway for the other. A reward of +5 is given to each agent for reaching their home cell. The episode ends when both agents has reached their home state or for a maximum of 100 steps in environment.\n",
        "\n",
        "Action Space: `0: Down, 1: Left, 2: Up , 3: Right, 4: Noop`\n",
        "\n",
        "Agent Observation : `Agent Coordinate + Steps in env.`\n",
        "\n",
        "Best Score: `NA`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bRNAhv1QMC1",
        "colab_type": "text"
      },
      "source": [
        "### Download Requirements and Set the Environment\n",
        "The following command will download the required scripts and set up the environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MTkFoNkLi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /content/ma-gym  \n",
        "!git clone https://github.com/koulanurag/ma-gym.git \n",
        "%cd /content/ma-gym \n",
        "!pip install -q -e . \n",
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install -U gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sYf6AK2d6kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import ma_gym\n",
        "from ma_gym.wrappers import Monitor\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOzRdQAOu7n_",
        "colab_type": "text"
      },
      "source": [
        "#### Example of playing Switch2-v0 Using Random Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXMuXU52pznF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"Switch2-v0\")) # Use \"Switch4-v0\" for the Switch-4 game\n",
        "done_n = [False for _ in range(env.n_agents)]\n",
        "ep_reward = 0\n",
        "\n",
        "obs_n = env.reset()\n",
        "while not all(done_n):\n",
        "    obs_n, reward_n, done_n, info = env.step(env.action_space.sample())\n",
        "    ep_reward += sum(reward_n)\n",
        "    env.render()\n",
        "env.close()\n",
        "# To improve the training efficiency, render() is not necessary during the training.\n",
        "# We provide the render and video code here just want to demonstrate how to debugging and analysis.\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHChldNnoWZ4",
        "colab_type": "text"
      },
      "source": [
        "Credit: Code above (Installation and random example) were written by Teaching Assistants from UCL's Multi Agent Artifical Intelligence course. All code below was written by me (Raymond Danks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22rC-6i4YztB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Requirements**\n",
        "* TensorFlow 2.0+ or PyTorch 1.4+ are recommended.\n",
        "*   Algorithm is Multi-Agent, i.e., policy input is the observation/ state for each corresponding agent, not for all agents.\n",
        "\n",
        "Training, Plotting, Testing and Explanation Included.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQxsW-I3hgG",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFYBKQ8Y3gpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(3517174048)\n",
        "import numpy as np\n",
        "st0 = np.random.get_state()\n",
        "#print(\"current seed?\",st0[0][1])\n",
        "import random\n",
        "random.seed(111)\n",
        "\n",
        "#The below is to stick the TensorFlow version, as concurrent updates can mess it up\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers\n",
        "from collections import deque\n",
        "from keras.layers import Activation, Dense\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64dJu3WwuBwu",
        "colab_type": "text"
      },
      "source": [
        "#### Code for Playing Switch2-v0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nQj9g0N6CrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN_Switch_v2:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.env = wrap_env(gym.make(\"Switch2-v0\"))\n",
        "    self.discount_rate = 0.9\n",
        "    self.batch_size = 32\n",
        "    self.training_episodes = 10000\n",
        "    self.max_timesteps = 50 #Already defined by gym\n",
        "    self.update_frequency = 32 #how often the networks are fitted\n",
        "    self.epsilon_max = 1 #what epsilon starts at (before annealing)\n",
        "    self.epsilon_projection = -0.2 #for linear interpolation\n",
        "    self.epsilon_min = 0.3 #makes sure epsilon does not go below this value\n",
        "    self.max_buffer_size = 100000\n",
        "    self.exploration_episodes = 0 #no training in exploration\n",
        "    self.testing_episodes = 1000 \n",
        "    self.test_epsilon = 0 #some papers claim that a ~5% randomness during testing is beneficial\n",
        "\n",
        "  def _Create_Model(self): # _ = private method.\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_dim=3, activation=\"relu\")) # coordinates AND timestep (early and late game strats are different)\n",
        "    model.add(Dense(5, activation=\"linear\")) #Outputs = amount of actions\n",
        "    model.compile(loss='mse',optimizer=\"Adam\") \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "  def _Update_Q_Policy(self,model,replay_batch):\n",
        "    #This function is to update the Q learning ANN model and hence update the Q policy\n",
        "\n",
        "    #The first step is to randomly sample some parts of the data batch:\n",
        "    #Our buffer has a large amount of moves, \n",
        "    #however our asynchronous databatch will just be a randomly chosen batch of these:\n",
        "    independent_batch = []\n",
        "    for i in range(self.batch_size): \n",
        "      random_batch_index = random.randint(0, len(replay_batch)-1) #select random row of the batch\n",
        "      random_batch = replay_batch[random_batch_index]\n",
        "      if i == 0:\n",
        "        independent_batch_messy = random_batch\n",
        "      else:\n",
        "        independent_batch_messy = np.vstack((independent_batch_messy,random_batch))\n",
        "    #Now we have a random batch of the appropriate size.\n",
        "\n",
        "    #columns of the batch go: state,action,new_state,reward,done, timestep\n",
        "\n",
        "    #reformat matrix batch first\n",
        "    independent_batch = np.zeros((self.batch_size,8))\n",
        "    for i in range(len(independent_batch_messy)):\n",
        "      first_coord_x = independent_batch_messy[i,0][0]\n",
        "      first_coord_y = independent_batch_messy[i,0][1]\n",
        "      action_int = independent_batch_messy[i,1]\n",
        "      second_coord_x = independent_batch_messy[i,2][0][0]\n",
        "      second_coord_y = independent_batch_messy[i,2][0][1]\n",
        "      reward_int = independent_batch_messy[i,2][1]\n",
        "      done_int = independent_batch_messy[i,2][2]\n",
        "      timestep_int = independent_batch_messy[i,2][3]\n",
        "\n",
        "      independent_batch[i,0] = first_coord_x \n",
        "      independent_batch[i,1] = first_coord_y\n",
        "      independent_batch[i,2] = action_int\n",
        "      independent_batch[i,3] = second_coord_x\n",
        "      independent_batch[i,4] = second_coord_y\n",
        "      independent_batch[i,5] = reward_int\n",
        "      independent_batch[i,6] = done_int\n",
        "      independent_batch[i,7] = timestep_int\n",
        "    #Batch is now fully formatted\n",
        "\n",
        "    #Firstly, we must gather  all of the current states:\n",
        "\n",
        "    #convert to batch size x 2 matrix for Keras\n",
        "    states = np.zeros((self.batch_size,3)) \n",
        "    for i in range(self.batch_size):\n",
        "      states[i,0] = independent_batch[i][0]\n",
        "      states[i,1] = independent_batch[i][1]\n",
        "      states[i,2] = independent_batch[i][7]\n",
        "    predicted_current_Q = model.predict(states) # this will output the Q values for all possible actions\n",
        "    \n",
        "    #now get the future states:\n",
        "    future_states = np.zeros((self.batch_size,3))\n",
        "    for i in range(self.batch_size):\n",
        "      future_states[i,0] = independent_batch[i,3]\n",
        "      future_states[i,1] = independent_batch[i,4]\n",
        "      future_states[i,2] = independent_batch[i,7]\n",
        "\n",
        "    predicted_future_Q = model.predict(future_states)\n",
        "\n",
        "    #Now we must define \"target\"; this is done slightly in our previous Q Learning system:\n",
        "    targets = []\n",
        "    for j in range(self.batch_size):\n",
        "      done = independent_batch[j,6]\n",
        "      reward = independent_batch[j,5]\n",
        "      if done:\n",
        "        target = reward\n",
        "      else:\n",
        "        V = np.max(predicted_future_Q[j])\n",
        "        target = reward+self.discount_rate*V\n",
        "\n",
        "      action_taken = independent_batch[j,2]\n",
        "\n",
        "      updated_Q_row = predicted_current_Q[j,:] #This is a row of the 5 Q values\n",
        "\n",
        "      updated_Q_row[int(action_taken)] = target #convert float -> int\n",
        "\n",
        "      if j == 0:\n",
        "        targets = target\n",
        "        updated_Q = updated_Q_row\n",
        "      else:\n",
        "        targets = np.vstack((targets,target))\n",
        "        updated_Q = np.vstack((updated_Q,updated_Q_row))\n",
        "      \n",
        "    #Above is updating our Q value with the new \"ideal\" values, which create the Y part for our fitting\n",
        "    #In the Keras notation, the above \"targets\" would be our y values\n",
        "    #Our current values are our X values, which are our current Q values!\n",
        "    #Assuming the network has done this all as one matrix!\n",
        "\n",
        "\n",
        "    X_model = np.zeros((self.batch_size,3)) \n",
        "    X_model[:,0] = independent_batch[:,0]\n",
        "    X_model[:,1] = independent_batch[:,1]\n",
        "    X_model[:,2] = independent_batch[:,7]\n",
        "\n",
        "    #This class is from the Keras website and allows for the loss to be recorded easily\n",
        "    class LossHistory(keras.callbacks.Callback): \n",
        "      def on_train_begin(self, logs={}):\n",
        "          self.losses = []\n",
        "\n",
        "      def on_batch_end(self, batch, logs={}):\n",
        "          self.losses.append(logs.get('loss'))\n",
        "\n",
        "    history = LossHistory()\n",
        "\n",
        "    history = model.fit(X_model,updated_Q, verbose = 0,batch_size = self.batch_size,epochs=200)\n",
        "    mean_loss = np.mean(history.history['loss'])\n",
        "    \n",
        "    return mean_loss \n",
        "    #loss shows how well the network is fit to current policy, not how good policy is.\n",
        "\n",
        "  #assigns the coordinates from the observation to individual agents and attaches the time step.\n",
        "  def _Observation_Conversion(self,observation,timestep): \n",
        "    observation_list_1 = observation[0]\n",
        "    observation_list_2 = observation[1]\n",
        "    \n",
        "    agent_1_x = observation_list_1[0]\n",
        "    agent_1_y = observation_list_1[1]\n",
        "    observation_agent_1 = np.zeros((1,3))\n",
        "    observation_agent_1[0,0] = agent_1_x\n",
        "    observation_agent_1[0,1] = agent_1_y\n",
        "    observation_agent_1[0,2] = timestep\n",
        "\n",
        "    agent_2_x = observation_list_2[0]\n",
        "    agent_2_y = observation_list_2[1]\n",
        "    observation_agent_2 = np.zeros((1,3))\n",
        "    observation_agent_2[0,0] = agent_2_x\n",
        "    observation_agent_2[0,1] = agent_2_y\n",
        "    observation_agent_2[0,2] = timestep\n",
        "    \n",
        "    return observation_agent_1, observation_agent_2\n",
        "\n",
        "\n",
        "  def DQN_Train(self):\n",
        "    env = self.env\n",
        "    done = [False for _ in range(env.n_agents)] #Instantiation of the \"done\" vector.\n",
        "    #above is a 1x2 array with boolean values, for each agent.\n",
        "\n",
        "    #recreate the models for each agent.\n",
        "    model_agent_1 = self._Create_Model()\n",
        "    model_agent_2 = self._Create_Model()\n",
        "\n",
        "\n",
        "    #Replay memory rows: state; action; new_state; reward; done; timestep\n",
        "    timesteps_required = [] #Should eventually be the same size as the maximum number of episodes\n",
        "    type_of_episode = [] #whether max iterations or convergence was reached\n",
        "    total_episode_rewards = []\n",
        "\n",
        "\n",
        "    buffer_switch = 0 #to keep track of when to instantiate the buffer\n",
        "    buffer_counter = 0 #when to activate the fit command - all timesteps together\n",
        "\n",
        "    #instantiate buffers\n",
        "    # replay_memory_agent_1 = deque(maxlen = self.max_buffer_size)\n",
        "    # replay_memory_agent_2 = deque(maxlen = self.max_buffer_size)\n",
        "\n",
        "    amount_of_episodes_maxed_out = 0 #instantiations\n",
        "    amount_of_episodes_completed = 0\n",
        "    all_episodic_rewards = []\n",
        "    buffer_row = 0 #start at zero\n",
        "\n",
        "    total_mean_loss_agent_1 = []\n",
        "    total_mean_loss_agent_2 = []\n",
        "    Episodes_Fitted = [] #for plotting loss\n",
        "\n",
        "    for episodes in range(self.training_episodes):\n",
        "      ep_reward = 0\n",
        "      observation = env.reset() #creates an initial observation\n",
        "      timestep = 0 #reinstantiation before each episode\n",
        "\n",
        "      if episodes < self.exploration_episodes:\n",
        "        epsilon = 1.1 \n",
        "      else:\n",
        "        #Now anneal epsilon until the end of the system\n",
        "        max_ep = self.training_episodes-self.exploration_episodes\n",
        "        ep = episodes-self.exploration_episodes\n",
        "        epsilon = ((max_ep-ep)/(max_ep))*(self.epsilon_max-self.epsilon_projection)+self.epsilon_projection #Linearly anneal epsilon\n",
        "        epsilon = np.max([self.epsilon_min,epsilon])\n",
        "      if episodes % 500 == 0:\n",
        "          print(\"episode \", episodes, \"out of \", self.training_episodes, \"complete.\")\n",
        "          print(\"Epsilon: \",epsilon)\n",
        "          print(\"amount of episodes maxed out: \", amount_of_episodes_maxed_out, \"out of \", episodes)\n",
        "          print(\"amount of episodes completed: \", amount_of_episodes_completed, \"out of \", episodes)\n",
        "      total_episodic_reward = 0\n",
        "      \n",
        "\n",
        "      while (1):  #exit criterion below\n",
        "        buffer_counter+=1 #overall amount of timesteps, across all episodes\n",
        "        timestep += 1 #progress timestep for each action\n",
        "        observation_agent_1, observation_agent_2 = self._Observation_Conversion(observation,timestep)\n",
        "\n",
        "        #Action for each agent MUST be determined by two individual networks,\n",
        "        #since they can't know each other's state (Multi-Agent)\n",
        "\n",
        "        #Defining the current Action\n",
        "        if  random.random() < epsilon: #random chance of random action based on epsilon\n",
        "            action = env.action_space.sample()  #random action\n",
        "        else:\n",
        "            possible_Q_values_agent_1 = model_agent_1.predict(observation_agent_1)\n",
        "            action_agent_1 = np.argmax(possible_Q_values_agent_1) # Pick the current policy's best action\n",
        "\n",
        "            possible_Q_values_agent_2 = model_agent_2.predict(observation_agent_2)\n",
        "            action_agent_2 = np.argmax(possible_Q_values_agent_2)\n",
        "\n",
        "            action = [action_agent_1,action_agent_2]\n",
        "\n",
        "        replay_memory_current_action_agent_1 = [observation[0],action[0]]\n",
        "        replay_memory_current_action_agent_2 = [observation[1],action[1]]\n",
        "\n",
        "        observation, reward, done, info = env.step(action)\n",
        "      \n",
        "        replay_memory_current_action_agent_1.append([observation[0],reward[0],done[0],timestep]) #observation is now next state\n",
        "        replay_memory_current_action_agent_2.append([observation[1],reward[1],done[1],timestep])\n",
        "\n",
        "        if buffer_counter == 1: #instantiation of buffers\n",
        "          replay_memory_agent_1 = replay_memory_current_action_agent_1\n",
        "          replay_memory_agent_2 = replay_memory_current_action_agent_2\n",
        "        elif len(replay_memory_agent_1) >= max_buffer_size: #If buffer is full then progressively replace older observations\n",
        "          replay_memory_agent_1[buffer_row,:] = replay_memory_current_action_agent_1\n",
        "          replay_memory_agent_2[buffer_row,:] = replay_memory_current_action_agent_2\n",
        "          if buffer_row == max_buffer_size-1: #-1 because it starts at 0\n",
        "            buffer_row = 0\n",
        "          else:\n",
        "            buffer_row += 1\n",
        "\n",
        "\n",
        "        else: #Building the replay memory\n",
        "          replay_memory_agent_1 = np.vstack((replay_memory_agent_1,replay_memory_current_action_agent_1))\n",
        "          replay_memory_agent_2 = np.vstack((replay_memory_agent_2,replay_memory_current_action_agent_2))\n",
        "\n",
        "        #Begin the fitting of the system - the data should be non-temporal and each row contains all info we need.\n",
        "        if buffer_counter % self.update_frequency == 0 and episodes >= self.exploration_episodes: \n",
        "          mean_loss_agent_1 = self._Update_Q_Policy(model_agent_1,replay_memory_agent_1)\n",
        "          mean_loss_agent_2 = self._Update_Q_Policy(model_agent_2,replay_memory_agent_2)\n",
        "          total_mean_loss_agent_1 = np.append(total_mean_loss_agent_1,mean_loss_agent_1)\n",
        "          total_mean_loss_agent_2 = np.append(total_mean_loss_agent_2,mean_loss_agent_2)\n",
        "          Episodes_Fitted = np.append(Episodes_Fitted,episodes)\n",
        "\n",
        "        ep_reward += sum(reward) #This is the reward for each timestep.\n",
        "        total_episodic_reward += ep_reward\n",
        "\n",
        "        if timestep >= self.max_timesteps: #do >= to avoid bugs - robustness!\n",
        "          timesteps_required = np.append(timesteps_required,timestep)\n",
        "          type_of_episode = np.append(type_of_episode,\"Max_Reached\")\n",
        "          total_episode_rewards = np.append(total_episode_rewards,ep_reward)\n",
        "          amount_of_episodes_maxed_out+=1\n",
        "          all_episodic_rewards = np.append(all_episodic_rewards,total_episodic_reward)\n",
        "          break\n",
        "        elif all(done): #stops each episode once the agents have completed the course\n",
        "          timesteps_required = np.append(timesteps_required,timestep)\n",
        "          type_of_episode = np.append(type_of_episode,\"Completed_Game\")\n",
        "          total_episode_rewards = np.append(total_episode_rewards,ep_reward)\n",
        "          amount_of_episodes_completed+=1\n",
        "          all_episodic_rewards = np.append(all_episodic_rewards,total_episodic_reward)\n",
        "          break\n",
        "    env.render() #added in for visualisation\n",
        "    env.close()\n",
        "    print(\"timesteps required: \",timesteps_required)\n",
        "    print(\"timesteps_required size: \",timesteps_required.size)\n",
        "    print(\"type of episodes: \",type_of_episode)\n",
        "    print(\"type of episode size: \",type_of_episode.size)\n",
        "    print(\"all rewards: \",total_episode_rewards)\n",
        "    print(\"all rewards size: \",total_episode_rewards.size)\n",
        "    print(\"amount of episodes maxed out: \", amount_of_episodes_maxed_out, \"out of \", self.training_episodes)\n",
        "    print(\"amount of episodes completed: \", amount_of_episodes_completed, \"out of \", self.training_episodes)\n",
        "    # To improve the training efficiency, render() is not necessary during the training.\n",
        "    # We provide the render and video code here just want to demonstrate how to debugging and analysis.\n",
        "    # show_video()\n",
        "\n",
        "    #now we set these variables for use in plotting:\n",
        "    self.final_rewards = total_episode_rewards\n",
        "    self.final_mean_losses_agent_1 = total_mean_loss_agent_1\n",
        "    self.final_mean_losses_agent_2 = total_mean_loss_agent_2\n",
        "    self.episodes_fitting_occurred = Episodes_Fitted #to make plotting more accurate\n",
        "\n",
        "\n",
        "\n",
        "  def DQN_Plot(self):\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "\n",
        "\n",
        "    number_of_episodes = len(all_episodic_rewards)\n",
        "    episode_matrix = np.arange(0,number_of_episodes)\n",
        "\n",
        "    ax1.plot(episode_matrix, self.final_rewards)\n",
        "    ax1.set_title(\"Learning Curve\")\n",
        "    ax1.set_ylabel(\"Episodic Reward (Cumulative for Both Agents)\")\n",
        "    ax1.set_xlabel(\"Episodes\")\n",
        "\n",
        "    ax2.plot(self.episodes_fitting_occurred, self.final_mean_losses_agent_1)\n",
        "    ax2.set_title(\"Loss for Agent 1\")\n",
        "    ax2.set_ylabel(\"Average Loss across epochs\")\n",
        "    ax2.set_xlabel(\"Episodes when Fitting occurred\")\n",
        "\n",
        "\n",
        "    ax3.plot(self.episodes_fitting_occurred, self.final_mean_losses_agent_1)\n",
        "    ax3.set_title(\"Loss for Agent 2\")\n",
        "    ax3.set_ylabel(\"Average Loss across epochs\")\n",
        "    ax3.set_xlabel(\"Episodes when Fitting occurred\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  #This is the same as the training code, except the networks are not updated, hence less comments\n",
        "  def Dqn_Test(self):\n",
        "    env = self.env\n",
        "    timestep = 0 #reinstantiation before each episode too\n",
        "    episodes_maxed_out_test = 0\n",
        "    episodes_completed_test = 0\n",
        "    total_episode_rewards_test = []\n",
        "    type_of_episode_test = []\n",
        "    timesteps_required_test = []\n",
        "\n",
        "    for episodes in range(self.testing_episodes):\n",
        "      timestep = 0\n",
        "      observation = env.reset() \n",
        "      ep_reward = 0\n",
        "      if episodes % 1000 == 0:\n",
        "        print(\"episode \",episodes,\"out of \",self.testing_episodes,\"Completed.\")\n",
        "      while (1):  #exit criterion below\n",
        "          timestep += 1 #progress timestep for each action\n",
        "          observation_agent_1, observation_agent_2 = self._Observation_Conversion(observation,timestep)\n",
        "\n",
        "          if random.random() < self.test_epsilon:\n",
        "            action = env.action_space.sample()\n",
        "          else:\n",
        "            possible_Q_values_agent_1 = model_agent_1.predict(observation_agent_1)\n",
        "            action_agent_1 = np.argmax(possible_Q_values_agent_1)\n",
        "\n",
        "            possible_Q_values_agent_2 = model_agent_2.predict(observation_agent_2)\n",
        "            action_agent_2 = np.argmax(possible_Q_values_agent_2)\n",
        "\n",
        "            action = [action_agent_1,action_agent_2]\n",
        "\n",
        "          observation, reward, done, info = env.step(action)\n",
        "          ep_reward += sum(reward)\n",
        "\n",
        "          if timestep >= self.max_timesteps: \n",
        "            # print(\"timesteps required: \",timestep)\n",
        "            type_of_episode_test = np.append(type_of_episode_test,\"Max_Reached\")\n",
        "            total_episode_rewards_test = np.append(total_episode_rewards_test,ep_reward)\n",
        "            episodes_maxed_out_test += 1\n",
        "            timesteps_required_test = np.append(timesteps_required_test,timestep)\n",
        "            # print(\"done: \",done)\n",
        "            break\n",
        "          elif all(done): #stops each episode once the agents have completed the course\n",
        "            type_of_episode_test = np.append(type_of_episode_test,\"Completed Game\")\n",
        "            total_episode_rewards_test = np.append(total_episode_rewards_test,ep_reward)\n",
        "            episodes_completed_test += 1\n",
        "            timesteps_required_test = np.append(timesteps_required_test,timestep)\n",
        "            break\n",
        "          env.render()\n",
        "\n",
        "    print(\"timesteps required: \",timesteps_required_test)\n",
        "    print(\"timesteps_required size: \",timesteps_required_test.size)\n",
        "    print(\"type of episodes: \",type_of_episode_test)\n",
        "    print(\"type of episode size: \",type_of_episode_test.size)\n",
        "    print(\"all rewards: \",total_episode_rewards_test)\n",
        "    print(\"all rewards size: \",total_episode_rewards_test.size)\n",
        "    print(\"amount of episodes maxed out: \", episodes_maxed_out_test, \"out of \", self.testing_episodes)\n",
        "    print(\"amount of episodes completed: \", episodes_completed_test, \"out of \", self.testing_episodes)\n",
        "\n",
        "    env.close()\n",
        "    # To improve the training efficiency, render() is not necessary during the training.\n",
        "    # We provide the render and video code here just want to demonstrate how to debugging and analysis.\n",
        "    show_video()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iin9WBe4uLY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DQN = DQN_Switch_v2()\n",
        "DQN.DQN_Train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_h9OFiBuIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DQN.DQN_Test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTZoC30suOe7",
        "colab_type": "text"
      },
      "source": [
        "#### Plot the Learning Curve and Agent Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgkfaU2RuRaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DQN.DQN_Plot()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}